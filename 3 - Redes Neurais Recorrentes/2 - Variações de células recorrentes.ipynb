{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2 - Variações de células recorrentes.ipynb","provenance":[],"authorship_tag":"ABX9TyPuf5j/dXWwIGZQVWgfdS5z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"y_kKqbJJvrqM"},"source":["# Variações de células recorrentes\n","\n","As arquiteturas recorrentes devem ser escolhidas de acordo com o problema a ser abordado:\n","\n","- **Many to one:** Arquitetura onde a entrada é uma sequência e a saída uma única inferência. Como na parte 1 que foi dado um nome (sequência de caracteres) de entrada e uma nacionalidade de saída.\n","\n","- **Many to many síncrono:** Arquitetura onde a entrada é uma sequência e a saída também, como pegar uma frase e encontrar os nomes próprios nessa frase. Dessa forma a saída possui correspondentes diretos com a entrada.\n","\n","- **Many to many assíncrono:** Arquitetura onde a entrada é uma sequência e a saída também. Como por exemplo o voice to text, onde a entrada é uma sequência de sons (voz) e a saída é o texto. Dessa forma não existe correspondência direta da entrada com a saída, como no many to many síncrono."]},{"cell_type":"markdown","metadata":{"id":"6TwYbqY5xwhH"},"source":["## O problema Vanishing Gradient\n","\n","As informações mais antigas vão sendo 'esquecidas' pela rede. Como na seguinte frase:\n","\n","- Larissa morou na França. Passou dois anos estudando lá. Hoje em dia ela fala muito bem ________. \n","\n","A rede deveria saber que a resposta certa é 'Francês'. Mas o conhecimento mais recente tende a pesar mais no cálculo do gradiente do que o conhecimento mais antigo.\n","\n","Dessa forma, a inferência é prejudicada em longas sequências. Mas como solucionar isso?\n","\n","### Solução: Gates\n","\n","Ferramental para interferir em como a memória é construída, para que o aprendizado mais antigo não tenha tão pouca influência.\n","\n","- São aplicados à memória interna da célula recorrente;\n","- Um 'portão' que decide quanto da informação vai passar adiante;\n","    - Nem sempre o passado mais recente é o mais importante;\n","    - Valores na faixa de 0 ~ 1 (ativação sigmoide); \n","\n","             | 0.9 | 0.6 | 0.2 |  -> Portão \n","                |     |     |        (% de info que será \n","                |     |     |        atribuída para cada h)\n","                |     |     |\n","             |  h1 |  h2 |  h3 |  -> Memória (da antiga para recente)\n","\n","- **GRU**: Gated Recurrent Unit. Substitui a RNNCell e é idêntica a implementação no pytorch;\n","    - Menos parâmetros;\n","    - Mais 'fácil' de treinar;\n","    - Desempenho semelhante à LSTM em boa parte das tarefas.\n","- **LSTM**: Long-Short Term Memory. Substitui o RNNCel mas possui um segundo vetor de memória, o cell state.\n","    - Mais parâmetros;\n","    - Mais 'difícil' de treinar;\n","    - Maior capacidade de representação.\n","\n","\n","Principal referência quando se fala em RNNs. Karpathy nos mostra algumas aplicações e visualizações do comportamento de modelos recorrentes simples.\n","http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n","\n","Uma versão animada do fluxo de dados nas unidades recorrentes. https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45\n","\n","Veja um post que detalha muito bem a função dos gates da LSTM. https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n"]}]}